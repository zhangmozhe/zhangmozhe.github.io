<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v9.1.6 <https://hydejack.com/>
--><head><title>Bo Zhang | Research Professor at Zhejiang University</title><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Bo Zhang" /><meta property="og:locale" content="en" /><meta name="description" content="Hi, I am currently a ZJU 100 Young Professor Zhejiang University. Previously, I served as a senior researcher of visual computing group at Microsoft research asia (MSRA) and deep learning research scientist in DeepSeek AI. My research interest involves generative networks, multimodal language model, embodied AI, computational photography and smart imaging system." /><meta property="og:description" content="Hi, I am currently a ZJU 100 Young Professor Zhejiang University. Previously, I served as a senior researcher of visual computing group at Microsoft research asia (MSRA) and deep learning research scientist in DeepSeek AI. My research interest involves generative networks, multimodal language model, embodied AI, computational photography and smart imaging system." /><link rel="canonical" href="https://bo-zhang.me/" /><meta property="og:url" content="https://bo-zhang.me/" /><meta property="og:site_name" content="Bo Zhang" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Bo Zhang" /><meta name="google-site-verification" content="GAV2aGzK3IPPyS7eR6V-obaNIa0-dWlq2vbG4ceA6Xk" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebSite","dateModified":"2023-03-31T12:49:58+08:00","description":"Hi, I am currently a ZJU 100 Young Professor Zhejiang University. Previously, I served as a senior researcher of visual computing group at Microsoft research asia (MSRA) and deep learning research scientist in DeepSeek AI. My research interest involves generative networks, multimodal language model, embodied AI, computational photography and smart imaging system.","headline":"Bo Zhang","name":"Bo Zhang - Zhejiang University","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://bo-zhang.me/assets/img/profile.jpg"}},"sameAs":["https://person.zju.edu.cn/TonyZhang","https://www.microsoft.com/en-us/research/people/zhanbo/","https://scholar.google.com/citations?user=PefHCMUAAAAJ&hl=en","https://www.linkedin.com/in/bo-zhang-8b753792/"],"url":"https://bo-zhang.me/"}</script><meta name="keywords" content="Bo,Zhang,Tony senior researcher,MSRA,HKUST,Microsoft research asia,Zhejiang University,DeepSeek AI,computer vision,microsoft,deep learning,personal website,personal webpage,computational photography,image synthesis"><meta name="theme-color" content="rgb(25,55,71)"><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Bo Zhang"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="application-name" content="Bo Zhang"><meta name="generator" content="Hydejack v9.1.6" /><link rel="alternate" href="https://bo-zhang.me/" hreflang="en"><link type="application/atom+xml" rel="alternate" href="https://bo-zhang.me/feed.xml" title="Bo Zhang" /><link rel="shortcut icon" href="/assets/icons/favicon.ico"><link rel="apple-touch-icon" href="/assets/icons/icon-192x192.png"><link rel="manifest" href="/assets/site.webmanifest"><link rel="dns-prefetch" href="https://fonts.googleapis.com"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preload" href="/assets/img/swipe.svg" as="image" id="_hrefSwipeSVG"> <script>!function(r,c){"use strict";function a(e,t,n,o){e.addEventListener?e.addEventListener(t,n,o):e.attachEvent?e.attachEvent("on"+t,n):e["on"+t]=n}r.loadJS=function(e,t){var n=c.createElement("script"),e=(n.src=e,t&&a(n,"load",t,{once:!0}),c.scripts[0]);return e.parentNode.insertBefore(n,e),n},r._loaded=!1,r.loadJSDeferred=function(e,t){var n=c.createElement("script");function o(){r._loaded=!0,t&&a(n,"load",t,{once:!0});var e=c.scripts[0];e.parentNode.insertBefore(n,e)}return n.src=e,r._loaded?o():a(r,"load",o,{once:!0}),n},r.setRel=r.setRelStylesheet=function(e){a(c.getElementById(e),"load",function(){this.rel="stylesheet"},{once:!0})}}(window,document); !function(a){"use strict";var b=function(b,c,d){function e(a){return h.body?a():void setTimeout(function(){e(a)})}function f(){i.addEventListener&&i.removeEventListener("load",f),i.media=d||"all"}var g,h=a.document,i=h.createElement("link");if(c)g=c;else{var j=(h.body||h.getElementsByTagName("head")[0]).childNodes;g=j[j.length-1]}var k=h.styleSheets;i.rel="stylesheet",i.href=b,i.media="only x",e(function(){g.parentNode.insertBefore(i,c?g:g.nextSibling)});var l=function(a){for(var b=i.href,c=k.length;c--;)if(k[c].href===b)return a();setTimeout(function(){l(a)})};return i.addEventListener&&i.addEventListener("load",f),i.onloadcssdefined=l,l(f),i};"undefined"!=typeof exports?exports.loadCSS=b:a.loadCSS=b}("undefined"!=typeof global?global:this); !function(a){if(a.loadCSS){var b=loadCSS.relpreload={};if(b.support=function(){try{return a.document.createElement("link").relList.supports("preload")}catch(b){return!1}},b.poly=function(){for(var b=a.document.getElementsByTagName("link"),c=0;c<b.length;c++){var d=b[c];"preload"===d.rel&&"style"===d.getAttribute("as")&&(a.loadCSS(d.href,d,d.getAttribute("media")),d.rel=null)}},!b.support()){b.poly();var c=a.setInterval(b.poly,300);a.addEventListener&&a.addEventListener("load",function(){b.poly(),a.clearInterval(c)}),a.attachEvent&&a.attachEvent("onload",function(){a.clearInterval(c)})}}}(this); !function(w) { w._baseURL = '/'; w._publicPath = '/assets/js/'; w._noPushState = false; w._noDrawer = false; w._noNavbar = false; w._noToc = false; w._noSearch = false; w._search = { DATA_URL: '/assets/sitedata.json?no-cache', STORAGE_KEY: 'mini-search/', INDEX_KEY: 'index--2024-05-11T20:50:28+08:00', }; w._clapButton = false; }(window);</script> <script async src="/assets/bower_components/MathJax/es5/tex-mml-chtml.js" id="_MathJax"></script> <!--[if gt IE 8]><!----><style id="_styleInline"> .clearfix,.sidebar-social::after{content:"";display:table;clear:both}.color-transition,.content .avatar,.nav-btn,.nav-btn-bar,.navbar,.message,.note-sm,#markdown-toc,.note,.hr-bottom,.hr-after::after,hr,.hr,p,body{transition:none}html{--font-family: system-ui, -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Oxygen, Ubuntu, Cantarell, Fira Sans, Droid Sans, Helvetica Neue, sans-serif;--font-family-heading: Roboto Slab, Helvetica, Arial, sans-serif;--code-font-family: Fira Code, Menlo, Monaco, Consolas, monospace;--body-color: #333;--body-bg: #fff;--border-color: #ebebeb;--gray: #777;--gray-bg: rgba(0, 0, 0, 0.025);--gray-text: #666;--menu-text: #bbb;--inv-body-color: #ccc;--inv-body-bg: #282828;--root-font-size: 15px;--root-font-size-medium: 16px;--root-font-size-large: 17px;--root-font-size-print: 8pt;--root-line-height: 1.75;--font-weight: 400;--font-weight-bold: 700;--font-weight-heading: 700;--content-width-5: 54rem;--content-margin-5: 4rem;--sidebar-width: 21rem;--half-content: 31rem;--break-point-3: 64em;--break-point-5: 86em;--break-point-dynamic: 1664px}html .content{-webkit-font-smoothing:initial;-moz-osx-font-smoothing:initial}*{box-sizing:border-box}html,body{margin:0;padding:0}html{font-family:var(--font-family);font-size:var(--root-font-size);line-height:var(--root-line-height)}body{color:var(--body-color);background-color:var(--body-bg);font-weight:var(--font-weight);overflow-y:scroll}.content img,.img,.content video,.video{max-width:100%;height:auto}.lead{margin-left:-1rem;margin-right:-1rem;margin-bottom:1.5rem}img.lead,video.lead{display:block;max-width:calc(100% + 2rem);width:calc(100% + 2rem);height:auto}.heading,.f6,h6,.h6,.f5,h5,.h5,.f4,.sidebar-nav-item,h4,.h4,.post-date,.f3,h3,.h3,.f2,h2,.h2,.f1,h1,.h1{font-family:var(--font-family-heading);font-weight:var(--font-weight-heading)}.f1,h1,.h1{font-size:2rem;line-height:1.3}.f2,h2,.h2{font-size:1.5rem;line-height:1.4}.f3,h3,.h3{font-size:1.2em;line-height:1.5}.f4,.sidebar-nav-item,h4,.h4,.post-date{font-size:1.08rem;line-height:1.6}.f5,h5,.h5{font-size:1.04rem;line-height:1.7}.f6,h6,.h6{font-size:1rem}.content h1>a,.content .h1>a{text-decoration:none;border-bottom:none}@media screen and (max-width: 42em){.content h1,.content .h1{font-size:1.7rem;line-height:1.35}}@media screen and (min-width: 86em){.content h1,.content .h1{font-size:2.4rem;line-height:1.25}}@media screen and (min-width: 1664px){body:not(.no-large-headings) .content h1,body:not(.no-large-headings) .content .h1{width:calc(100% + 50vw - 32rem);font-size:3rem;line-height:1.2}}@media screen and (min-width: 124em){body:not(.no-large-headings) .content h1,body:not(.no-large-headings) .content .h1{font-size:4rem;line-height:1.1}}h1,h2,h3,.h1,.h2,.h3{margin:4rem 0 1rem}h4,h5,h6,.h4,.post-date,.h5,.h6{margin:3rem 0 .5rem}p{margin-top:0;margin-bottom:1rem}p.lead{font-size:1.2em;margin-top:1.5rem;margin-bottom:1.5rem;padding:0 1rem}ul,ol,dl{margin-top:0;margin-bottom:1rem}ul,ol{padding-left:1.25rem}hr,.hr{border:0;margin:1rem 0;border-top:1px solid var(--border-color)}.hr-after::after{content:"";display:block;margin:1rem 0;border-top:1px solid var(--border-color)}.hr-bottom{border-bottom:1px solid var(--border-color);padding-bottom:.75rem;margin-bottom:1rem}.page{margin-bottom:3em}.page li+li{margin-top:.25rem}.page>header{position:relative;margin-bottom:2rem}@media screen and (min-width: 1664px){body:not(.no-third-column) .page>header>.lead+.note-sm,body:not(.no-third-column) .page>header>.lead+#markdown-toc,body:not(.no-third-column) .page>header>.lead+.note,body:not(.no-third-column) .page>header>a.no-hover+.note-sm,body:not(.no-third-column) .page>header>a.no-hover+#markdown-toc,body:not(.no-third-column) .page>header>a.no-hover+.note{position:absolute;right:-25rem;width:21rem;bottom:0;margin-bottom:0}}.page-title,.post-title{margin-top:0}.post-date{display:flex;justify-content:space-between;margin-top:-0.6rem;height:2rem;margin-bottom:.85rem;color:var(--gray)}.post-date>.ellipsis,#breadcrumbs.post-date>ul{cursor:pointer}.post-date [class^=icon-]{display:inline-block;font-size:smaller;margin-right:.25rem}.related-posts{padding-left:0;list-style:none;margin-bottom:2rem}.related-posts>li,.related-posts>li+li{margin-top:1rem}.message,.note-sm,#markdown-toc,.note{margin-bottom:1rem;padding:1rem;color:var(--gray-text);background-color:var(--gray-bg);margin-left:-1rem;margin-right:-1rem}.note-sm,#markdown-toc,.note{background:rgba(0,0,0,0);color:var(--body-color);font-size:smaller;border-left:1px solid var(--border-color);padding:1.2rem 1rem 0 1rem;margin:1rem -1rem;position:relative}.note-sm:before,#markdown-toc:before,.note:before{font-size:.667rem;font-weight:bold;font-style:normal;letter-spacing:.025rem;text-transform:uppercase;color:var(--menu-text);position:absolute;top:0}.note-sm[title]:before,[title]#markdown-toc:before,[title].note:before{content:attr(title) !important}.note{font-size:1rem}@media screen{body::before{content:"";width:.5rem;background:var(--gray-bg);position:fixed;left:0;top:0;bottom:0}}@media(min-width: 64em){body::before{width:21rem}}@media(min-width: 1664px){body::before{width:calc(50% - 31rem)}}@media screen and (min-width: 42em){html{font-size:var(--root-font-size-medium)}}@media screen and (min-width: 124em){html{font-size:var(--root-font-size-large)}}#breadcrumbs>ul{height:1rem;margin:-1.5rem 0 .5rem;padding:0;font-size:.667rem;color:var(--menu-text);text-transform:uppercase;width:100%;list-style:none}#breadcrumbs>ul>li{display:inline}#breadcrumbs>ul>li a{color:var(--gray);text-decoration:none;border-bottom:none}.fl{float:left}.fr{float:right}.mb4{margin-bottom:4rem}.mb6{margin-bottom:6rem}.mt0{margin-top:0}.mt1{margin-top:1rem}.mt2{margin-top:2rem}.mt3{margin-top:3rem}.mt4{margin-top:4rem}.pb0{padding-bottom:0}.ml1{margin-left:1rem}.mr1{margin-right:1rem}.sixteen-nine{position:relative}.sixteen-nine::before{display:block;content:"";width:100%;padding-top:56.25%}.sixteen-nine>*{position:absolute;top:0;left:0;right:0;bottom:0}.sixteen-ten{position:relative}.sixteen-ten::before{display:block;content:"";width:100%;padding-top:62.5%}.sixteen-ten>*{position:absolute;top:0;left:0;right:0;bottom:0}.four-three{position:relative}.four-three::before{display:block;content:"";width:100%;padding-top:75%}.four-three>*{position:absolute;top:0;left:0;right:0;bottom:0}.sr-only{display:none}.larger{font-size:larger}.smaller{font-size:smaller}.clearfix,.sidebar-social::after{content:"";display:table;clear:both}.ellipsis,#breadcrumbs>ul{white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.border{border:1px solid var(--border-color)}@media(min-width: 42em){.border-radius,.lead,.page .aspect-ratio.sixteen-nine.lead{border-radius:.5rem}}.fallback-img{background-position:center;background-repeat:no-repeat;background-image:url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPHN2ZyB3aWR0aD0iMTYwIiBoZWlnaHQ9IjkwIiB2aWV3Qm94PSIwIDAgMTYwIDkwIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogIDxnIHRyYW5zZm9ybT0ibWF0cml4KDAuMDQ4ODI4LCAwLCAwLCAwLjA0Nzk5MSwgNTQuOTk5OTczLCAyMC40MjgxNDgpIj4KICAgIDxwYXRoIHN0eWxlPSJmaWxsOnJnYmEoMTI4LDEyOCwxMjgsLjMzKSIgZD0iTTk1OS44ODQgMTI4YzAuMDQwIDAuMDM0IDAuMDgyIDAuMDc2IDAuMTE2IDAuMTE2djc2Ny43N2MtMC4wMzQgMC4wNDAtMC4wNzYgMC4wODItMC4xMTYgMC4xMTZoLTg5NS43N2MtMC4wNDAtMC4wMzQtMC4wODItMC4wNzYtMC4xMTQtMC4xMTZ2LTc2Ny43NzJjMC4wMzQtMC4wNDAgMC4wNzYtMC4wODIgMC4xMTQtMC4xMTRoODk1Ljc3ek05NjAgNjRoLTg5NmMtMzUuMiAwLTY0IDI4LjgtNjQgNjR2NzY4YzAgMzUuMiAyOC44IDY0IDY0IDY0aDg5NmMzNS4yIDAgNjQtMjguOCA2NC02NHYtNzY4YzAtMzUuMi0yOC44LTY0LTY0LTY0djB6Ii8+CiAgICA8cGF0aCBzdHlsZT0iZmlsbDpyZ2JhKDEyOCwxMjgsMTI4LC4zMykiIGQ9Ik04MzIgMjg4YzAgNTMuMDIwLTQyLjk4IDk2LTk2IDk2cy05Ni00Mi45OC05Ni05NiA0Mi45OC05NiA5Ni05NiA5NiA0Mi45OCA5NiA5NnoiLz4KICAgIDxwYXRoIHN0eWxlPSJmaWxsOnJnYmEoMTI4LDEyOCwxMjgsLjMzKSIgZD0iTTg5NiA4MzJoLTc2OHYtMTI4bDIyNC0zODQgMjU2IDMyMGg2NGwyMjQtMTkyeiIvPgogIDwvZz4KPC9zdmc+")}hy-push-state a{color:var(--body-color)}@supports not ((text-decoration-thickness: initial) and (text-underline-offset: initial)){hy-push-state a{text-decoration:none;border-bottom:2px solid}}@supports(text-decoration-thickness: initial) and (text-underline-offset: initial){hy-push-state a{text-decoration-style:solid;text-underline-offset:.35rem;text-decoration-thickness:2px}}hy-push-state a.no-hover{border-bottom:none;text-decoration-thickness:unset;text-underline-offset:unset}.content a:not(.btn):not(.no-hover){border-color:var(--accent-color-faded)}@supports(text-decoration-thickness: initial) and (text-underline-offset: initial){.content a:not(.btn):not(.no-hover){text-decoration-color:var(--accent-color-faded)}}.content .aspect-ratio{overflow:hidden}.content .aspect-ratio img{margin:0;width:100%;height:100%;background-color:var(--gray-bg)}hy-drawer{width:100%;position:relative;overflow:hidden;display:block;z-index:4}@media screen and (min-width: 64em){hy-drawer{position:fixed;width:21rem;top:0;left:0;bottom:0;margin-left:0}hy-drawer.cover{position:relative;width:100%}}@media screen and (min-width: 1664px){hy-drawer{width:calc(50% - 31rem)}}.sidebar{position:relative;display:flex;justify-content:center;align-items:center;color:rgba(255,255,255,.75);text-align:center;min-height:100vh}.sidebar.invert{color:rgba(32,32,32,.75)}.sidebar a{color:#fff;border-bottom-color:rgba(255,255,255,.2);text-decoration-color:rgba(255,255,255,.2)}.sidebar.invert a{color:#222;border-bottom-color:rgba(32,32,32,.2);text-decoration-color:rgba(32,32,32,.2)}.sidebar-bg{position:absolute;top:0;left:calc(50% - 50vw);width:100vw;height:100%;background:#202020 center/cover}.sidebar-bg::after{content:"";position:absolute;top:0;left:0;bottom:0;right:0;background:rgba(0,0,0,.05)}.sidebar-bg.sidebar-overlay::after{background:linear-gradient(to bottom, rgba(32, 32, 32, 0) 0%, rgba(32, 32, 32, 0.5) 50%, rgba(32, 32, 32, 0) 100%)}.sidebar-sticky{position:relative;z-index:3;max-width:21rem;padding:1.5rem;contain:content}.sidebar-about .avatar{margin-bottom:1.5rem}.sidebar-about>a.sidebar-title{text-decoration:none}.sidebar-about>a.sidebar-title>h2{margin:0;padding-bottom:.5rem}.sidebar-about>a.sidebar-title::after{content:"";display:block;border-bottom:2px solid;margin:0 auto .5rem;width:4rem;border-color:rgba(255,255,255,.2);transition:border-color 250ms}.sidebar-about>a.sidebar-title:hover::after{border-color:#fff;transition:border-color 50ms}.sidebar.invert .sidebar-about>a.sidebar-title::after{border-color:rgba(32,32,32,.2)}.sidebar.invert .sidebar-about>a.sidebar-title:hover::after{border-color:#222}.sidebar-nav>ul{list-style:none;padding-left:0}.sidebar-nav-item{display:inline-block;margin-bottom:.5rem}@media(min-width: 64em){#_main.no-drawer #_menu{display:none}#_main.no-drawer .nav-btn-bar>:nth-child(2){border:none}}.sidebar-social>ul{display:inline-block;list-style:none;padding-left:0;margin-bottom:0}.sidebar-social>ul>li{float:left}.sidebar-social>ul>li>a{display:inline-block;text-align:center;font-size:1.4rem;width:3rem;height:4rem;padding:.5rem 0;line-height:3rem;text-decoration:none;border-bottom-width:2px;border-bottom-style:solid}.sidebar-social>ul li+li{margin-top:0}.fixed-common,.fixed-bottom,.fixed-top{position:fixed;left:0;width:100%;z-index:2}.fixed-top{top:0}.fixed-bottom{bottom:0}.navbar>.content{position:relative;padding-top:0;padding-bottom:0;min-height:0;max-height:5rem}.nav-btn-bar{margin:0 -1rem;background-color:#fff;background-color:var(--body-bg);height:5rem;display:flex;align-items:center;position:relative}.nav-btn-bar>:first-child,.nav-btn-bar>:last-child{border:none}.nav-btn{background:none;border:none;text-decoration:none;display:flex;align-items:center;justify-content:center;width:3.25rem;height:5rem;color:var(--menu-text);border-right:1px solid var(--border-color);border-left:1px solid var(--border-color);margin-left:-1px}#markdown-toc{margin:2rem -1rem 2rem calc(-1rem + 1px);padding-left:2.5rem;padding-bottom:.5rem}#markdown-toc:before{left:1rem}@media screen and (min-width: 1664px){body:not(.no-toc) #markdown-toc{position:absolute;z-index:4;width:20.5rem;right:0;margin:auto;overflow:auto}}@media screen and (min-width: 1664px){body.no-break-layout:not(.no-toc) #markdown-toc{width:calc(50% - 31rem)}}.content{margin-left:auto;margin-right:auto;padding:8rem 1rem 12rem}@media screen{.content{padding-left:1.5rem;min-height:100vh}}@media screen and (min-width: 42em){.content{max-width:42rem}}@media screen and (min-width: 54em){.content{max-width:48rem}}@media screen and (min-width: 64em){.content{padding-left:1rem;margin-left:24rem;margin-right:3rem}}@media screen and (min-width: 86em){.content{padding-top:9rem;margin-left:25rem;margin-right:4rem;max-width:54rem}}@media screen and (min-width: 1664px){.content{margin:auto}}.large-only{display:none}@media screen and (min-width: 1664px){.large-only{display:block}}.avatar{width:7rem;height:7rem;border-radius:100%;overflow:hidden;display:inline-block}.avatar img{width:100%}.content .avatar{float:right;box-sizing:content-box;border:1rem solid var(--body-bg);transition:border-color 1s ease;margin-top:-1.5rem;margin-right:-1rem}.sidebar a{text-shadow:rgba(0,0,0,.25) .1rem .1rem .15rem}.note:before{content:"Note"}.page>header>.note-sm:before,.page>header>.note:before,.page>header>#markdown-toc:before{content:"Description"}#markdown-toc:before{content:"Table of Contents"}.layout-resume .note-sm:before,.layout-resume .note:before,.layout-resume #markdown-toc:before{content:"Summary"}html{--accent-color: rgb(68,78,83);--accent-color-faded: rgba(68, 78, 83, 0.5);--accent-color-highlight: rgba(68, 78, 83, 0.1);--accent-color-darkened: #333a3e;--theme-color: rgb(25,55,71)}</style><link rel="preload" as="style" href="/assets/css/hydejack-9.1.6.css" id="_stylePreload"><link rel="preload" as="style" href="/assets/icomoon/style.css" id="_iconsPreload"><link rel="preload" as="style" href="https://fonts.googleapis.com/css?family=Roboto+Slab:700%7CNoto+Sans:400,400i,700,700i&display=swap" id="_fontsPreload"> <script> setRel('_stylePreload'); setRel('_iconsPreload'); /**/setRel('_fontsPreload');/**/ </script> <noscript><link rel="stylesheet" href="/assets/css/hydejack-9.1.6.css"><link rel="stylesheet" href="/assets/icomoon/style.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:700%7CNoto+Sans:400,400i,700,700i&display=swap"> </noscript> <!--<![endif]--> <!-- Code used for embedding Gumroad on the Hydejack Site. Left here for reference, feel free to delete. --><link rel="dns-prefetch" href="https://assets.gumroad.com"> <script> window.GET_CLAPS_API = 'https://worker.getclaps.dev' </script> <script type="module"> const ppiData = window._ppiData = fetch('https://hydejack-ppi.qwtel.workers.dev', { mode: 'cors'}).then(res => res.json()).catch(() => ({})); const promisify = f => x => new Promise(r => f(x).addEventListener('load', r)); const loadJS = promisify(window.loadJS); { let p; document.querySelector('hy-push-state').addEventListener('load', () => { const io = new IntersectionObserver(async (entries) => { if (entries.some(x => x.isIntersecting)) { p = p || loadJS('https://gumroad.com/js/gumroad-embed.js'); const [{ code }] = await Promise.all([ppiData, p]); document.querySelectorAll('.gumroad-product-embed').forEach(el => { if (!el.dataset.gumroadParams) { el.dataset.gumroadParams = 'offer_code=' + (code || 'qr0tw8m'); } }); if (!window.GumroadEmbed) { await new Promise(function check1(res) { if ('createGumroadEmbed' in window) res(window.createGumroadEmbed()); else setTimeout(() => check1(res), 200); }); } await new Promise(function check2(res) { if ('GumroadEmbed' in window) res(GumroadEmbed.reload()); else setTimeout(() => check2(res), 200); }); } }, { rootMargin: '1440px' }); document.querySelectorAll('.gumroad-product-embed').forEach(el => io.observe(el)); }); } { let p; document.querySelector('hy-push-state').addEventListener('load', () => { const io = new IntersectionObserver(async (entries) => { if (entries.some(x => x.isIntersecting)) { p = p || loadJS('https://gumroad.com/js/gumroad.js'); const [{ code }] = await Promise.all([ppiData, p]); if (code) { document.querySelectorAll('.gumroad-button').forEach(el => { if (!el.href.endsWith(code)) { el.href = el.href + '/' + (code || 'qr0tw8m'); } }); } if (!window.GumroadOverlay) { await new Promise(function check(res) { if ('createGumroadOverlay' in window) res(window.createGumroadOverlay()); else setTimeout(() => check(res), 200); }); } } }, { rootMargin: '300px' }); document.querySelectorAll('.gumroad-button').forEach(el => io.observe(el)); }); } </script> <script src="/assets/js/bib_script.js"></script><body class="no-break-layout"> <hy-push-state id="_pushState" replace-selector="#_main" link-selector="a[href]:not([href^='/assets/']):not(.external):not(.no-push-state)" script-selector="script" duration="500" hashchange > <hy-drawer id="_drawer" class="cover" side="left" threshold="10" noscroll opened ><header id="_sidebar" class="sidebar" role="banner"><div class="sidebar-bg sidebar-overlay" style="background-color:rgb(25,55,71);background-image:url(/assets/img/bg.jpg)"></div><div class="sidebar-sticky"><div class="sidebar-about"> <a class="no-hover" href="/" tabindex="-1"> <img src="/assets/img/profile.jpg" class="avatar" alt="Bo Zhang" width="120" height="120" loading="lazy" /> </a> <a class="sidebar-title" href="/"><h2 class="h1">Bo Zhang</h2></a><p class=""> Research Professor at Zhejiang University</div><nav class="sidebar-nav heading" role="navigation"> <span class="sr-only">Navigation:</span><ul><li> <a id="_drawer--opened" href="/#about" class="sidebar-nav-item " > About </a><li> <a href="/#publications" class="sidebar-nav-item " > Publications </a><li> <a href="/#services" class="sidebar-nav-item " > Services </a><li> <a href="/#honors" class="sidebar-nav-item " > Honors & Awards </a></ul></nav><div class="sidebar-social"> <span class="sr-only">Social:</span><ul><li> <a href="mailto:bo.zhang@zju.edu.cn" title="Email" class="no-mark-external"> <span class="icon-mail"></span> <span class="sr-only">Email</span> </a><li> <a href="https://scholar.google.com/citations?user=PefHCMUAAAAJ&hl" title="Google Scholar" class="no-mark-external"> <span class="icon-googlescholar"></span> <span class="sr-only">Google Scholar</span> </a><li> <a href="https://github.com/zhangmozhe" title="GitHub" class="no-mark-external"> <span class="icon-github"></span> <span class="sr-only">GitHub</span> </a><li> <a href="https://www.linkedin.com/in/bo-zhang-8b753792" title="LinkedIn" class="no-mark-external"> <span class="icon-linkedin2"></span> <span class="sr-only">LinkedIn</span> </a><li> <a href="https://twitter.com/zhangboknight" title="Twitter" class="no-mark-external"> <span class="icon-twitter"></span> <span class="sr-only">Twitter</span> </a></ul></div></div></header></hy-drawer><hr class="sr-only" hidden /><div id="_navbar" class="navbar fixed-top"><div class="content"> <span class="sr-only">Jump to:</span><div class="nav-btn-bar"> <a id="_menu" class="nav-btn no-hover" href="#_drawer--opened"> <span class="sr-only">Navigation</span> <span class="icon-menu"></span> </a><div class="nav-span"></div></div></div></div><hr class="sr-only" hidden /><main id="_main" class="content layout-plain" role="main" ><nav id="breadcrumbs" class="screen-only"><ul></ul></nav><article class="page mb6" role="article"><header><h1 class="page-title">Bo Zhang</h1><div class="hr pb0"></div></header><style type="text/css"> .page-title { position: absolute; width: 1px; height: 1px; margin: -1px; border: 0; padding: 0; clip: rect(0 0 0 0); overflow: hidden; }</style><h2 class="h1" style="color: rgb(1,92,171)" id="about">Short Bio</h2><p>I am currently a ZJU 100 Young Professor (Ph.D. supervisor) of Zhejiang University. Previously I served as a senior researcher at <a href="https://www.microsoft.com/en-us/research/group/visual-computing/">Visual Computing Group</a> of <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a> (MSRA) and AI research scientist at <a href="https://www.deepseek.com/">DeepSeek</a>.<p>I received my Ph.D. degree with the Department of Electronic and Computer Engineering at Hong Kong University of Science and Technology (<a href="https://hkust.edu.hk/">HKUST</a>) in 2019. Prior to that, I received my Bachelor degree of Engineering at <a href="http://www.zju.edu.cn/english/">Zhejiang University</a> in 2013. I joined the Microsoft Research Asia in May 2019.<p>My research interest involves 2D/3D content creation, virtual human modeling, multimodal models, and embodied intelligence. My work has made contributions to the field of content generation, such as the high-quality image translation CoCosNet series (with CoCosNet v2 being a CVPR 2021 Best Paper nominee), the industry’s first text-to-image generation diffusion model VQ-Diffusion, the first high-quality 3D diffusion generation model Rodin, the 3D generation technology DreamCraft 3D, and the well-known open-source multimodal large model DeepSeek-VL. Additionally, our work “Bringing Old Photos Back to Life” was listed as one of the top 30 AI advancements in 2020 by the renowned AI media louisbouchard.ai.<p>We are always open to welcoming self-motivated PhD candidates, Master’s and Bachelor’s students, as well as postdocs and research assistants. Besides, we are also looking for research collaboration with industry and research lab. Feel free to reach out at <a href="mailto:bo.zhang@zju.edu.cn">bo.zhang@zju.edu.cn</a>.<p>For prospective students, I am looking for:<ul><li><strong>Positive Attitude</strong>: I hope you are a nice person; ambitious, caring, energetic, and responsible. I look forward to us resonating well together.<li><strong>Strong Mathematical Skills</strong>: Ability to abstract problems and a solid foundation in mathematics.<li><strong>Excellent Programming Skills</strong>: Capability to quickly implement ideas and perform necessary, albeit sometimes “dirty”, but important work.<li><strong>Quality over Quantity</strong>: Willingness to invest time in producing high-quality work that you can be proud of—aiming for work that counts as three.<li><strong>Ownership of Your PhD</strong>: This is your PhD. Take full responsibility right from the start and avoid a dependency mindset to empower yourself.<li><strong>Commitment to Creation</strong>: Willingness to genuinely create (like composing music à la Mozart), rather than picking the low-hanging fruit or chasing after metrics.</ul><div class="body-social sidebar-social"><ul><li> <a href="https://person.zju.edu.cn/TonyZhang" title="Google Scholar" class="no-mark-external" target="_blank"> <span class="icon-link"></span> <span aria-hidden="true">ZJU homepage </span><span class="sr-only">Bo Zhang's ZJU homepage</span></a><li> <a href="https://www.microsoft.com/en-us/research/people/zhanbo/" title="Google Scholar" class="no-mark-external" target="_blank"> <span class="icon-link"></span> <span aria-hidden="true">Microsoft homepage </span><span class="sr-only">Bo Zhang's Microsoft homepage</span></a><li> <a href="https://scholar.google.com/citations?user=PefHCMUAAAAJ&amp;hl=en" title="Google Scholar" class="no-mark-external" target="_blank"> <span class="icon-googlescholar"></span> <span aria-hidden="true">Google Scholar </span><span class="sr-only">Bo Zhang's Google Scholar profile</span></a><li> <a href="https://twitter.com/zhangboknight" title="Twitter" class="no-mark-external" target="_blank"> <span class="icon-twitter"></span> <span aria-hidden="true">Twitter </span><span class="sr-only">Bo Zhang's Twitter</span></a><li> <a href="https://www.linkedin.com/in/bo-zhang-8b753792/" title="LinkedIn" class="no-mark-external" target="_blank"> <span class="icon-linkedin2"></span> <span aria-hidden="true">LinkedIn </span><span class="sr-only">Bo Zhang's LinkedIn profile</span></a></ul></div><hr /><h2 class="h1" style="color: rgb(1,92,171)" id="publications">Publications</h2><p>(†Intern student, *Equal Contribution)<ul><li><p>DeepSeek AI. “DeepSeek-VL: Towards Real-World Vision-Language Understanding”. (<strong>Project lead of DeepSeek-VL</strong>) [<a href="https://github.com/deepseek-ai/DeepSeek-VL">project</a>][<a href="https://arxiv.org/abs/2403.05525">paper</a>] 🔥 (prestigious multimodal language model)<li><p>DeepSeek AI. “DeepSeek LLM: Scaling Open-source Language Models with Longtermism”. [<a href="https://github.com/deepseek-ai/DreamCraft3D">project</a>][<a href="https://arxiv.org/abs/2401.02954">paper</a>] (prestigious open-source large language model)<li><p>Jingxiang Sun†, <strong>Bo Zhang</strong><sup>✉</sup>, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, Yebin Liu<sup>✉</sup>. “DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior”, International Conference on Learning Representations (ICLR 2024). [<a href="https://github.com/deepseek-ai/DreamCraft3D">project</a>][<a href="https://arxiv.org/abs/2310.16818">paper</a>][<a href="####" onclick="toggle_visibility('dreamcraft3d');">bibtex</a>] <code class="bibtex" id="dreamcraft3d">{sun2023dreamcraft3d,<br /> title={Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior},<br /> author={Sun, Jingxiang and Zhang, Bo and Shao, Ruizhi and Wang, Lizhen and Liu, Wen and Xie, Zhenda and Liu, Yebin}, <br /> journal={arXiv preprint arXiv:2310.16818}, <br /> year={2023}<br /> }</code><li><p>Junshu Tang†, Tengfei Wang†, <strong>Bo Zhang</strong><sup>✉</sup>, Ting Zhang, Ran Yi, Lizhuang Ma, Dong Chen”<strong>Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior</strong>”, arXiv preprint. [<a href="https://make-it-3d.github.io/">project</a>][<a href="https://arxiv.org/abs/2303.14184">paper</a>][<a href="####" onclick="toggle_visibility('make-it-3d');">bibtex</a>] <code class="bibtex" id="make-it-3d">{tang2023make,<br /> title={Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior},<br /> author={Tang, Junshu and Wang, Tengfei and Zhang, Bo and Zhang, Ting and Yi, Ran and Ma, Lizhuang and Chen, Dong},<br /> journal={arXiv preprint arXiv:2303.14184},<br /> year={2023}<br /> }</code><li><p>Tengfei Wang†*, <strong>Bo Zhang</strong>*<sup>✉</sup>, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, Baining Guo. “<strong>Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion</strong>”, 2023 IEEE Conference on Computer Vision and Pattern Recognition <b style="color: rgb(220,129,0)">(CVPR 2023 Highlight)</b>. [<a href="https://3d-avatar-diffusion.microsoft.com/">project</a>][<a href="https://arxiv.org/abs/2212.06135">paper</a>][<a href="####" onclick="toggle_visibility('rodin');">bibtex</a>] <code class="bibtex" id="rodin">{wang2022rodin,<br /> title={Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion}, <br /> author={Tengfei Wang and Bo Zhang and Ting Zhang and Shuyang Gu and Jianmin Bao and Tadas Baltrusaitis and Jingjing Shen and Dong Chen and Fang Wen and Qifeng Chen and Baining Guo},<br /> journal={arXiv preprint arXiv:2212.06135},<br /> year={2022}<br /> }</code><li><p>Binxin Yang†, Shuyang Gu, <strong>Bo Zhang</strong><sup>✉</sup>, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen. “<strong>Paint by Example: Exemplar-based Image Editing with Diffusion Models</strong>”, 2023 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2023). [<a href="https://arxiv.org/abs/2211.13227">paper</a>][<a href="https://github.com/fantasy-studio/paint-by-example">code</a>][<a href="https://huggingface.co/spaces/Fantasy-Studio/Paint-by-Example">demo</a>][<a href="####" onclick="toggle_visibility('paint_by_example');">bibtex</a>] <code class="bibtex" id="paint_by_example">@article{yang2022paint,<br /> title={Paint by Example: Exemplar-based Image Editing with Diffusion Models},<br /> author={Yang, Binxin and Gu, Shuyang and Zhang, Bo and Zhang, Ting and Chen, Xuejin and Sun, Xiaoyan and Chen, Dong and Wen, Fang},<br /> journal={arXiv preprint arXiv:2211.13227},<br /> year={2022}<br /> }</code><li><p>Bowen Zhang†*, Chenyang Qi, Pan Zhang, <strong>Bo Zhang</strong><sup>✉</sup>, HsiangTao Wu, Dong Chen, Qifeng Chen, Yong Wang, Fang Wen. “<strong>MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation</strong>”, 2023 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2023). [<a href="https://meta-portrait.github.io/">project</a>][<a href="https://arxiv.org/abs/2212.08062">paper</a>][<a href="####" onclick="toggle_visibility('metaportrait');">bibtex</a>] <code class="bibtex" id="metaportrait">@article{zhang2022metaportrait,<br /> title={MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation},<br /> author={Zhang, Bowen and Qi, Chenyang and Zhang, Pan and Zhang, Bo and Wu, HsiangTao and Chen, Dong and Chen, Qifeng and Wang, Yong and Wen, Fang},<br /> journal={arXiv preprint arXiv:2212.08062},<br /> year={2022}<br /> }</code><li><p>Junshu Tang†, <strong>Bo Zhang</strong><sup>✉</sup>, Binxin Yang†, Ting Zhang, Dong Chen, Lizhuang Ma<sup>✉</sup>, Fang Wen. “<strong>3DFaceShop: Explicitly Controllable 3D-Aware Portrait Generation</strong>”, arXiv preprint. [<a href="https://junshutang.github.io/control/index.html">project</a>][<a href="https://arxiv.org/abs/2209.05434">paper</a>][<a href="https://github.com/junshutang/3DFaceShop">code</a>][<a href="####" onclick="toggle_visibility('3dfaceshop');">bibtex</a>] <code class="bibtex" id="3dfaceshop">@article{tang2022explicitly,<br /> title={Explicitly Controllable 3D-Aware Portrait Generation},<br /> author={Tang, Junshu and Zhang, Bo and Yang, Binxin and Zhang, Ting and Chen, Dong and Ma, Lizhuang and Wen, Fang},<br /> journal={arXiv preprint arXiv:2209.05434},<br /> year={2022}<br /> }</code><li><p>Tengfei Wang†, Ting Zhang, <strong>Bo Zhang</strong><sup>✉</sup>, Hao Ouyang†, Dong Chen, Qifeng Chen, Fang Wen. “<strong>Pretraining is All You Need for Image-to-Image Translation</strong>”, arXiv preprint. [<a href="https://tengfei-wang.github.io/PITI/index.html">project</a>][<a href="https://arxiv.org/abs/2205.12952">paper</a>][<a href="https://github.com/PITI-Synthesis/PITI">code</a>][<a href="https://huggingface.co/spaces/tfwang/piti-synthesis">demo</a>][<a href="####" onclick="toggle_visibility('pretrain_diffusion');">bibtex</a>] <code class="bibtex" id="pretrain_diffusion">@article{wang2022pretraining,<br /> title={Pretraining is All You Need for Image-to-Image Translation},<br /> author={Wang, Tengfei and Zhang, Ting and Zhang, Bo and Ouyang, Hao and Chen, Dong and Chen, Qifeng and Wen, Fang},<br /> journal={arXiv preprint arXiv:2205.12952},<br /> year={2022}<br /> }</code><li><p>Hao Ouyang†, <strong>Bo Zhang</strong><sup>✉</sup>, Pan Zhang†, Hao Yang, Jiaolong Yang, Dong Chen, Qifeng Chen<sup>✉</sup>, Fang Wen. “<strong>Real-Time Neural Character Rendering with Pose-Guided Multiplane Images</strong>”, European Conference on Computer Vision (ECCV 2022). [<a href="https://ken-ouyang.github.io/cmpi/index.html">project</a>][<a href="https://arxiv.org/abs/2204.11820">paper</a>][<a href="https://github.com/ken-ouyang/PGMPI">code</a>][<a href="https://www.youtube.com/watch?v=otl3uaak7wq">youtube video</a>][<a href="####" onclick="toggle_visibility('avatar_mpi');">bibtex</a>] <code class="bibtex" id="avatar_mpi">@article{ouyang2022real, title={Real-Time Neural Character Rendering with Pose-Guided Multiplane Images},<br /> author={Ouyang, Hao and Zhang, Bo and Zhang, Pan and Yang, Hao and Yang, Jiaolong and Chen, Dong and Chen, Qifeng and Wen, Fang},<br /> journal={arXiv preprint arXiv:2204.11820},<br /> year={2022}<br /> }</code><li><p>Bowen Zhang†, Shuyang Gu†, <strong>Bo Zhang</strong><sup>✉</sup>, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, Baining Guo. “<strong>StyleSwin: Transformer-based GAN for High-resolution Image Generation</strong>”, 2022 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2022). [<a href="https://arxiv.org/abs/2112.10762">paper</a>][<a href="https://github.com/microsoft/styleswin">code</a>][<a href="https://huggingface.co/spaces/hysts/StyleSwin">demo</a>][<a href="####" onclick="toggle_visibility('styleswin');">bibtex</a>] <code class="bibtex" id="styleswin">@article{zhang2021styleswin,<br /> title={StyleSwin: Transformer-based GAN for High-resolution Image Generation},<br /> author={Bowen Zhang and Shuyang Gu and Bo Zhang and Jianmin Bao and Dong Chen and Fang Wen and Yong Wang and Baining Guo},<br /> journal={arXiv preprint arXiv:2112.10762},<br /> year={2021}<br /> }</code><li><p>Shuyang Gu†, Dong Chen, Jianmin Bao, Fang Wen, <strong>Bo Zhang</strong>, Dongdong Chen, Lu Yuan, Baining Guo. “<strong>Vector Quantized Diffusion Model for Text-to-Image Synthesis</strong>”, 2022 IEEE Conference on Computer Vision and Pattern Recognition <b style="color: rgb(220,129,0)">(CVPR 2022 Oral)</b>. [<a href="https://arxiv.org/abs/2111.14822">paper</a>][<a href="https://github.com/microsoft/vq-diffusion">code<a>][<a href="https://huggingface.co/docs/diffusers/api/pipelines/vq_diffusion">Huggingface API</a>][<a href="https://huggingface.co/blog/vq-diffusion">huggingface blog</a>][<a href="####" onclick="toggle_visibility('vq_diffusion');">bibtex</a>] <code class="bibtex" id="vq_diffusion">@article{gu2021vector,<br /> title={Vector Quantized Diffusion Model for Text-to-Image Synthesis},<br /> author={Gu, Shuyang and Chen, Dong and Bao, Jianmin and Wen, Fang and Zhang, Bo and Chen, Dongdong and Yuan, Lu and Guo, Baining},<br /> journal={arXiv preprint arXiv:2111.14822},<br /> year={2021}<br /> }</code></a></a><li><p>Ziyu Wan, <strong>Bo Zhang</strong>, Dongdong Chen, Jing Liao. “<strong>Bringing Old Films Back to Life</strong>”, 2022 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2022). [<a href="http://raywzy.com/Old_Film/">project</a>][<a href="https://arxiv.org/abs/2203.17276">paper</a>][<a href="https://github.com/raywzy/Bringing-Old-Films-Back-to-Life">code</a>][<a href="####" onclick="toggle_visibility('old_film');">bibtex</a>] <code class="bibtex" id="old_film">@inproceedings{wan2022bringing, title={Bringing Old Films Back to Life},<br /> author={Wan, Ziyu and Zhang, Bo and Chen, Dongdong and Liao, Jing},<br /> booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br /> pages={17694--17703},<br /> year={2022}<br /> }</code><li><p>Pan Zhang†, <strong>Bo Zhang</strong><sup>✉</sup>, Ting Zhang, Dong Chen, Fang Wen. “<strong>Robust Mutual Learning for Semi-supervised Semantic Segmentation</strong>”, arXiv preprint, May 2021. [<a href="https://arxiv.org/abs/2106.00609v1">paper</a>][<a href="####" onclick="toggle_visibility('neurips2021');">bibtex</a>] <code class="bibtex" id="neurips2021">@article{zhang2021robust,<br /> title={Robust Mutual Learning for Semi-supervised Semantic Segmentation},<br /> author={Zhang, Pan and Zhang, Bo and Zhang, Ting and Chen, Dong and Wen, Fang},<br /> journal={arXiv preprint arXiv:2106.00609},<br /> year={2021}<br /> }</code><li><p>Xiaoyu Li, <strong>Bo Zhang</strong><sup>✉</sup>, Jing Liao, Pedro V. Sander. “<strong>Let’s See Clearly: Contaminant Artifact Removal for Moving Cameras</strong>”, 2021 International Conference on Computer Vision (ICCV 2021). [<a href="https://arxiv.org/abs/2104.08852">paper</a>][<a href="####" onclick="toggle_visibility('iccv2021');">bibtex</a>] <code class="bibtex" id="iccv2021">@article{li2021let,<br /> title={Let's See Clearly: Contaminant Artifact Removal for Moving Cameras},<br /> author={Li, Xiaoyu and Zhang, Bo and Liao, Jing and Sander, Pedro V},<br /> journal={arXiv preprint arXiv:2104.08852},<br /> year={2021}<br /> }</code><li><p>Chulin Xie†*, Chuxin Wang†*, <strong>Bo Zhang</strong><sup>✉</sup>, Hao Yang, Dong Chen, Fang Wen. “<strong>Style-based Point Generator with Adversarial Rendering for Point Cloud Completion</strong>”, 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021). [<a href="https://alphapav.github.io/SpareNet/">project</a>][<a href="https://arxiv.org/abs/2103.02535">paper</a>][<a href="https://github.com/microsoft/SpareNet">code</a>][<a href="####" onclick="toggle_visibility('sparenet');">bibtex</a>] <code class="bibtex" id="sparenet">@article{xie2021style,<br /> title={Style-based Point Generator with Adversarial Rendering for Point Cloud Completion},<br /> author={Xie, Chulin and Wang, Chuxin and Zhang, Bo and Yang, Hao and Chen, Dong and Wen, Fang},<br /> journal={arXiv preprint arXiv:2103.02535},<br /> year={2021}<br /> }</code><li><p>Xingran Zhou†, <strong>Bo Zhang</strong><sup>✉</sup>, Ting Zhang, Pan Zhang, Jianmin Bao, Dong Chen, Zhongfei Zhang, Fang Wen. “<strong>Full-Resolution Correspondence Learning for Image Translation</strong>”, 2021 IEEE Conference on Computer Vision and Pattern Recognition <b style="color: rgb(220,129,0)">(CVPR 2021 Oral, best paper candidate)</b>. [<a href="https://arxiv.org/abs/2012.02047">paper</a>][<a href="https://github.com/microsoft/cocosnet-v2">code</a>][<a href="####" onclick="toggle_visibility('focosnet');">bibtex</a>] <code class="bibtex" id="focosnet">@article{zhou2020full,<br /> title={Full-Resolution Correspondence Learning for Image Translation},<br /> author={Zhou, Xingran and Zhang, Bo and Zhang, Ting and Zhang, Pan and Bao, Jianmin and Chen, Dong and Zhang, Zhongfei and Wen, Fang},<br /> journal={arXiv preprint arXiv:2012.02047},<br /> year={2020}<br /> }</code><li><p>Pan Zhang†, <strong>Bo Zhang</strong><sup>✉</sup>, Ting Zhang, Dong Chen, Yong Wang, Fang Wen. “<strong>Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation</strong>”, 2021 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2021). [<a href="https://arxiv.org/abs/2101.10979">paper</a>][<a href="https://github.com/microsoft/proda/">code</a>][<a href="####" onclick="toggle_visibility('proda');">bibtex</a>] <code class="bibtex" id="proda">@article{zhang2021prototypical,<br /> title={Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation},<br /> author={Zhang, Pan and Zhang, Bo and Zhang, Ting and Chen, Dong and Wang, Yong and Wen, Fang},<br /> journal={arXiv preprint arXiv:2101.10979},<br /> year={2021}<br /> }</code><li><p>Xiaoyu Li, <strong>Bo Zhang</strong>, Jing Liao, Pedro V. Sander. “<strong>Deep Sketch-guided Cartoon Video Inbetweening</strong>”, IEEE transactions on Visualization and Computer Graphics (TVCG 2021). [<a href="https://arxiv.org/abs/2008.04149">paper</a>][<a href="https://github.com/xiaoyu258/inbetweening">code</a>][<a href="https://www.youtube.com/watch?v=qeNcjlAFqVo">Youtube</a>][<a href="####" onclick="toggle_visibility('inbetweening');">bibtex</a>] <code class="bibtex" id="inbetweening">@article{li2021deep,<br /> title={Deep Sketch-guided Cartoon Video Inbetweening},<br /> author={Li, Xiaoyu and Zhang, Bo and Liao, Jing and Sander, Pedro},<br /> journal={IEEE Transactions on Visualization and Computer Graphics},<br /> year={2021},<br /> publisher={IEEE}<br /> }</code><li><p>Ziyu Wan†, <strong>Bo Zhang</strong>, Dongdong Chen, Pan Zhang, Dong Chen, Jing Liao, Fang Wen. “<strong>Old Photo Restoration via Deep Latent Space Translation</strong>”, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022. [<a href="https://arxiv.org/abs/2009.07047">paper</a>][<a href="https://github.com/microsoft/bringing-old-photos-back-to-life">code</a>](<em>the algorithm now supports high-resolution restoration</em>.)[<a href="####" onclick="toggle_visibility('old_photo_journal');">bibtex</a>] <code class="bibtex" id="old_photo_journal">@article{wan2020old,<br /> title={Old Photo Restoration via Deep Latent Space Translation},<br /> author={Wan, Ziyu and Zhang, Bo and Chen, Dongdong and Zhang, Pan and Chen, Dong and Liao, Jing and Wen, Fang},<br /> journal={arXiv preprint arXiv:2009.07047},<br /> year={2020}<br /> }</code><li><p>Pan Zhang†, <strong>Bo Zhang</strong><sup>✉</sup>, Dong Chen, Lu Yuan, Fang Wen. “<strong>Cross-domain Correspondence Learning for Exemplar-based Image Translation</strong>”, 2020 IEEE Conference on Computer Vision and Pattern Recognition <b style="color: rgb(220,129,0)">(CVPR 2020 Oral)</b>. [<a href="https://panzhang0212.github.io/CoCosNet/">project</a>][<a href="https://panzhang0212.github.io/cocosnet/">paper</a>][<a href="https://github.com/microsoft/CoCosNet">code</a>][<a href="https://www.dropbox.com/s/g7dezxm2mhw6gqo/cocosnet%20slides.pptx?dl=0">slides</a>][<a href="####" onclick="toggle_visibility('cocosnet');">bibtex</a>] <code class="bibtex" id="cocosnet">@inproceedings{zhang2020cross,<br /> title={Cross-domain correspondence learning for exemplar-based image translation},<br /> author={Zhang, Pan and Zhang, Bo and Chen, Dong and Yuan, Lu and Wen, Fang},<br /> booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br /> pages={5143--5153},<br /> year={2020}<br /> }</code><li><p>Ziyu Wan†, <strong>Bo Zhang</strong>, Dongdong Chen, Pan Zhang, Dong Chen, Jing Liao, Fang Wen. “<strong>Bringing Old Photos Back to Life</strong>”, 2020 IEEE Conference on Computer Vision and Pattern Recognition <b style="color: rgb(220,129,0)">(CVPR 2020 Oral)</b>. [<a href="http://raywzy.com/Old_Photo/">project</a>][<a href="https://arxiv.org/abs/2004.09484">paper</a>][<a href="https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life">code</a>][<a href="https://drive.google.com/file/d/10cctmu06yfhackflwkv4dfq5aqktueff/view">supplementary</a>](<i>Welcome to try our <a href="https://colab.research.google.com/drive/1NEm6AsybIiC5TwTU_4DqDkQO0nFRB-uA?usp=sharing">Colab demo</a></i>)[<a href="####" onclick="toggle_visibility('old_photo');">bibtex</a>] <code class="bibtex" id="old_photo">@inproceedings{wan2020bringing,<br /> title={Bringing old photos back to life},<br /> author={Wan, Ziyu and Zhang, Bo and Chen, Dongdong and Zhang, Pan and Chen, Dong and Liao, Jing and Wen, Fang},<br /> booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={2747--2757},<br /> year={2020}<br /> }</code><li><p>Xiaoyu Li, <strong>Bo Zhang</strong>, Jing Liao, Pedro V. Sander. “<strong>Document Image Rectification using a Patch-based CNN</strong>”, ACM Transactions on Graphics 38(6), 168:1-168:11 (Siggraph Asia 2019). [<a href="https://xiaoyu258.github.io/projects/docproj/">project</a>][<a href="https://arxiv.org/abs/1909.09470">paper</a>][<a href="https://github.com/xiaoyu258/DocProj">code</a>][<a href="####" onclick="toggle_visibility('li2019document');">bibtex</a>] <code class="bibtex" id="li2019document">@article{li2019document,<br /> title={Document rectification and illumination correction using a patch-based CNN},<br /> author={Li, Xiaoyu and Zhang, Bo and Liao, Jing and Sander, Pedro V},<br /> journal={ACM Transactions on Graphics (TOG)},<br /> volume={38},<br /> number={6},<br /> pages={1--11},<br /> year={2019},<br /> publisher={ACM New York, NY, USA}<br /> }</code><li><p><strong>Bo Zhang</strong>, Jing Liao, Pedro V. Sander, Amine Bermak. “<strong>Deep Exemplar-based Video Colorization</strong>”, 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019). [<a href="https://arxiv.org/abs/1906.09909">paper</a>][<a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/03/cvpr19_poster_bo.pptx">slides</a>][<a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/03/cvpr19_poster_Bo.pdf">poster</a>][<a href="https://github.com/zhangmozhe/video-colorization">code</a>][<a href="https://www.youtube.com/watch?v=HXWR5h5vVYI">youtube video demo</a>][<a href="####" onclick="toggle_visibility('zhang2019deep');">bibtex</a>] <code class="bibtex" id="zhang2019deep">@inproceedings{zhang2019deep,<br /> title={Deep exemplar-based video colorization},<br /> author={Zhang, Bo and He, Mingming and Liao, Jing and Sander, Pedro V and Yuan, Lu and Bermak, Amine and Chen, Dong},<br /> booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br /> pages={8052--8061},<br /> year={2019}<br /> }</code><li><p>Xiaoyu Li, <strong>Bo Zhang</strong>, Jing Liao, Pedro V. Sander. “<strong>Blind Geometric Distortion Correction on Images Through Deep Learning</strong>”, 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019). [<a href="https://xiaoyu258.github.io/projects/geoproj/">project</a>][<a href="http://openaccess.thecvf.com/content_cvpr_2019/papers/li_blind_geometric_distortion_correction_on_images_through_deep_learning_cvpr_2019_paper.pdf">paper</a>][<a href="https://github.com/xiaoyu258/GeoProj">code</a>][<a href="####" onclick="toggle_visibility('li2019blind');">bibtex</a>] <code class="bibtex" id="li2019blind">@inproceedings{li2019blind,<br /> title={Blind geometric distortion correction on images through deep learning},<br /> author={Li, Xiaoyu and Zhang, Bo and Sander, Pedro V and Liao, Jing},<br /> booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br /> pages={4855--4864},<br /> year={2019}<br /> }</code><li><p><strong>Bo Zhang</strong>, Pedro V. Sander, Chi-Ying Tsui and Amine Bermak. “<strong>Microshift: An Efficient Image Compression Algorithm for Hardware</strong>”, IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2018. [<a href="https://ieeexplore.ieee.org/document/8529272">paper</a>][<a href="https://github.com/zhangmozhe/microshift_compression/blob/master/paper.pdf">arxiv</a>][<a href="https://github.com/zhangmozhe/microshift_compression">code</a>][<a href="####" onclick="toggle_visibility('zhang2018microshift');">bibtex</a>] <code class="bibtex" id="zhang2018microshift">@article{zhang2018microshift,<br /> title={Microshift: An efficient image compression algorithm for hardware},<br /> author={Zhang, Bo and Sander, Pedro V and Tsui, Chi-Ying and Bermak, Amine},<br /> journal={IEEE Transactions on Circuits and Systems for Video Technology},<br /> volume={29},<br /> number={11},<br /> pages={3430--3443},<br /> year={2018},<br /> publisher={IEEE}<br /> }</code><li><p>Xiaopeng Zhong, <strong>Bo Zhang</strong>, Amine Bermak, Chi-Ying Tsui, Man-Kay Law. “<strong>A Low-Power Compression-Based CMOS Image Sensor With Microshift-Guided SAR ADC</strong>”, IEEE Transactions on Circuits and Systems II (TCAS-II), 2018. [<a href="https://ieeexplore.ieee.org/document/8418781">paper</a>][<a href="####" onclick="toggle_visibility('zhong2018low');">bibtex</a>] <code class="bibtex" id="zhong2018low">@article{zhong2018low,<br /> title={A low-power compression-based CMOS image sensor with microshift-guided SAR ADC},<br /> author={Zhong, Xiaopeng and Zhang, Bo and Bermak, Amine and Tsui, Chi-Ying and Law, Man-Kay},<br /> journal={IEEE Transactions on Circuits and Systems II: Express Briefs},<br /> volume={65},<br /> number={10},<br /> pages={1350--1354},<br /> year={2018},<br /> publisher={IEEE}<br /> }</code><li><p><strong>Bo Zhang</strong>, Pedro V. Sander, Amine Bermak. “<strong>Registration Based Retargeted Image Quality Assessment</strong>”, in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017). [<a href="https://ieeexplore.ieee.org/document/7952358">paper</a>][<a href="https://github.com/zhangmozhe/retarget_iqa">code</a>][<a href="####" onclick="toggle_visibility('zhang2017registration');">bibtex</a>] <code class="bibtex" id="zhang2017registration">@inproceedings{zhang2017registration,<br /> title={Registration based retargeted image quality assessment},<br /> author={Zhang, Bo and Sander, Pedro V and Bermak, Amine},<br /> booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},<br /> pages={1258--1262},<br /> year={2017},<br /> organization={IEEE}<br /> }</code><li><p><strong>Bo Zhang</strong>, Pedro V. Sander, Amine Bermak. “<strong>Gradient Magnitude Similarity Deviation On Multiple Scales For Color Image Quality Assessment</strong>”, in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017). <a href="Integrated into &lt;a href=&quot;https://github.com/photosynthesis-team/piq&quot;&gt;PIQ&lt;a&gt; library"><a href="https://ieeexplore.ieee.org/document/7952357">paper</a></a>[<a href="####" onclick="toggle_visibility('zhang2017gradient');">bibtex</a>] <code class="bibtex" id="zhang2017gradient">@inproceedings{zhang2017gradient,<br /> title={Gradient magnitude similarity deviation on multiple scales for color image quality assessment},<br /> author={Zhang, Bo and Sander, Pedro V and Bermak, Amine},<br /> booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},<br /> pages={1253--1257},<br /> year={2017},<br /> organization={IEEE}<br /> }</code><li><p><strong>Bo Zhang</strong>, Xiaopeng Zhong, Bo Wang, Pedro V. Sander, and Amine Bermak. “<strong>Wide Dynamic Range PSD algorithms and Their Implementation for Compressive Imaging</strong>”, in 2016 IEEE International Symposium on Circuits and Systems (ISCAS 2016). [<a href="https://ieeexplore.ieee.org/document/7539156">paper</a>][<a href="####" onclick="toggle_visibility('zhang2016wide');">bibtex</a>] <code class="bibtex" id="zhang2016wide">@inproceedings{zhang2016wide,<br /> title={Wide dynamic range PSD algorithms and their implementation for compressive imaging},<br /> author={Zhang, Bo and Zhong, Xiaopeng and Wang, Bo and Sander, Pedro V and Bermak, Amine},<br /> booktitle={2016 IEEE International Symposium on Circuits and Systems (ISCAS)},<br /> pages={2727--2730},<br /> year={2016},<br /> organization={IEEE}<br /> }</code><li><p>Xiaopeng Zhong, <strong>Bo Zhang</strong>, and Amine Bermak. “<strong>A Background Subtraction based Column-parallel Analog-to-information Converter for Motion-triggered Vision Sensor</strong>”, in 2016 IEEE International Symposium on Circuits and Systems (ISCAS 2016). [<a href="https://ieeexplore.ieee.org/document/7527518/">paper</a>][<a href="####" onclick="toggle_visibility('zhong2016background');">bibtex</a>] <code class="bibtex" id="zhong2016background">@inproceedings{zhong2016background,<br /> title={A background subtraction based column-parallel analog-to-information converter for motion-triggered vision sensor},<br /> author={Zhong, Xiaopeng and Zhang, Bo and Bermak, Amine},<br /> booktitle={2016 IEEE International Symposium on Circuits and Systems (ISCAS)},<br /> pages={1426--1429},<br /> year={2016},<br /> organization={IEEE}<br /> }</code><li><p>Liu, Dong, Yongying Yang, Zhongtao Cheng, Hanlu Huang, <strong>Bo Zhang</strong>, Tong Ling, and Yibing Shen. “<strong>Retrieval and analysis of a polarized high-spectral-resolution lidar for profiling aerosol optical properties</strong>”, Optics Express, 2013. [<a href="https://www.osapublishing.org/DirectPDFAccess/ADB87C05-042E-616B-AD867FEAF8C63B00_253755/oe-21-11-13084.pdf?da=1&amp;id=253755&amp;seq=0&amp;mobile=no">paper</a>][<a href="####" onclick="toggle_visibility('liu2013retrieval');">bibtex</a>] <code class="bibtex" id="liu2013retrieval">@article{liu2013retrieval,<br /> title={Retrieval and analysis of a polarized high-spectral-resolution lidar for profiling aerosol optical properties},<br /> author={Liu, Dong and Yang, Yongying and Cheng, Zhongtao and Huang, Hanlu and Zhang, Bo and Ling, Tong and Shen, Yibing},<br /> journal={Optics express},<br /> volume={21},<br /> number={11},<br /> pages={13084--13093},<br /> year={2013},<br /> publisher={Optical Society of America}<br /> }</code><li><p>Liu, Dong, Yongying Yang, Zhongtao Cheng, Hanlu Huang, <strong>Bo Zhang</strong>, and Yibing Shen. “<strong>Development of the ZJU polarized near-infrared high spectral resolution lidar</strong>”, in International Symposium on Photoelectronic Detection and Imaging 2013. [<a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/8905/1/Development-of-the-ZJU-polarized-near-infrared-high-spectral-resolution/10.1117/12.2035435.full?SSO=1">paper</a>][<a href="####" onclick="toggle_visibility('liu2013development');">bibtex</a>] <code class="bibtex" id="liu2013development">@inproceedings{liu2013development,<br /> title={Development of the ZJU polarized near-infrared high spectral resolution lidar},<br /> author={Liu, Dong and Yang, Yongying and Cheng, Zhongtao and Huang, Hanlu and Zhang, Bo and Shen, Yibing},<br /> booktitle={International Symposium on Photoelectronic Detection and Imaging 2013: Laser Sensing and Imaging and Applications},<br /> volume={8905},<br /> pages={89052W},<br /> year={2013},<br /> organization={International Society for Optics and Photonics}<br /> }</code></ul><hr /><h2 class="h1" style="color: rgb(1,92,171)" id="services">Academic Services</h2><h3 class="h2">Conference Reviewer</h3><p> IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<br /> International Conference on Computer Vision (ICCV)<br /> European Conference on Computer Vision (ECCV)<br /> ACM SIGGRAPH<br /> ACM SIGGRAPH Asia<br /> IEEE Winter Conference on Application of Computer Vision (WACV)<h3 class="h2">Journal Reviewer</h3><p> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br /> International Journal on Computer Vision (IJCV)<br /> ACM Transactions on Graphics (TOG)<br /> IEEE Transactions on Image Processing (TIP)<br /> IEEE Transactions on Visualization and Computer Graphics (TVCG) <br /> IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)<br /> IEEE Transactions on Multimedia (TMM)<br /> Computer Vision and Image Understanding (CVIU)<br /> The Visual Computer (TVCJ)<br /> Neural Computing<br /> Visual informatics<br /><h2 class="h1" style="color: rgb(1,92,171)" id="honors">Honors and Awards</h2><p> Tencent Rhino-bird Research Program (腾讯犀牛鸟计划)<br /> Excellent award, Stars of Tomorrow Internship Program, Microsoft Research Asia (MSRA)<br /> Graduate Research Scholarship of HKUST<br /> Outstanding Undergraduate of Zhejiang University<br /> Outstanding Undergraduate of Zhejiang Province, China<br /> Bronze medal, International RoboCup 2013 competition<br /> First prize, Optical Science Technology Competition of Zhejiang Province<br /> Meritorious Winner (First prize), International Mathematical Contest in Modeling (MCM)<br /> First prize, China Undergraduate Mathematical Contest in Modeling (CUMCM)<br /> First prize, Undergraduate Physics Innovation Competition of Zhejiang Province<br /> First prize, Undergraduate Calculus Competition of Zhejiang Province, China<br /> <script type="text/javascript"> function toggle_visibility(id) { var e = document.getElementById(id); if(e.style.display == 'block') e.style.display = 'none'; else e.style.display = 'block'; } </script><style type="text/css"> .body-social > ul { display: inline-block; list-style-type: none; margin-bottom: 0; overflow: hidden; padding: 0; } .body-social > ul > li { float: left; /* padding-left: 5px; */ padding-right: 10px; /* display: inline-block; */ } .body-social > ul > li > a { display: inline; text-align: center; font-size: 0.95rem; font-weight: 600; /*width: 3rem;*/ /*height: 4rem;*/ padding: 4px; /* line-height: 3rem; */ text-decoration: none; border-width: 1px; border-style: solid; border-radius: 5px; transition: background-color 250ms, color 250ms, text-decoration-color 250ms, border-color 250ms; /* border-bottom: none; */ } .body-social > ul > li > a:not(.btn):not(.no-hover) { border-color: var(--accent-color); } .body-social > ul > li > a:hover { color: white; background-color: var(--accent-color); border-radius: 5px; padding: 4px; transition: background-color 250ms, color 250ms, text-decoration-color 250ms, border-color 250ms; } .bibtex { display: none; }</style></article><hr class="dingbat related mb6" /><footer class="content" role="contentinfo"><hr/><p><small class="copyright">© 2021 Bo Zhang. All rights reserved. </small><nav class="legal"><small> <a class="heading flip-title" href="/LICENSE/">LICENSE</a> </small></nav><p><small>Powered by <a class="external" href="https://hydejack.com/">Hydejack</a> v<span id="_version">9.1.6</span></small><hr class="sr-only"/></footer></main></hy-push-state> <!--[if gt IE 10]><!----> <script nomodule>!function(){var t,n=document.createElement("script");!("noModule"in n)&&"onbeforeload"in n&&(t=!1,document.addEventListener("beforeload",function(e){if(e.target===n)t=!0;else if(!e.target.hasAttribute("nomodule")||!t)return;e.preventDefault()},!0),n.type="module",n.src=".",document.head.appendChild(n),n.remove())}(); </script> <script src="/assets/js/hydejack-9.1.6.js" type="module"></script> <script src="/assets/js/LEGACY-hydejack-9.1.6.js" nomodule defer></script> <script>!function(w, d) { w.ga=w.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; /**/ ga('create', 'UA-190657022-1', 'auto'); /**/ var pushStateEl = d.getElementById('_pushState'); var timeoutId; pushStateEl.addEventListener('hy-push-state-load', function() { w.clearTimeout(timeoutId); timeoutId = w.setTimeout(function() { ga('set', 'page', w.location.pathname); ga('send', 'pageview'); }, 500); }); d.addEventListener('hy--cookies-ok', function () { w.ga(function(tracker) { w.ga("set", "anonymizeIp", undefined); localStorage && localStorage.setItem("ga--client-id", tracker.get("clientId")); }); }); w.loadJSDeferred('https://www.google-analytics.com/analytics.js'); }(window, document);</script> <!--<![endif]--> <!-- Code for integrating CloudFlare's email protection with Hydejack's single page app loading. --> <script> document.getElementById('_pushState').addEventListener('hy-push-state-after', function (e) { function e(e){ (console.error?console.error:console.log).call(console,e) } function t(e){ return l.innerHTML='<a href="'+e.replace(/"/g,"&quot;")+'"></a>',l.childNodes[0].getAttribute("href") } function r(e,t){ var r=e.substr(t,2);return parseInt(r,16) } function n(e,n){ for(var o="",c=r(e,n),a=n+2;a<e.length;a+=2){ var l=r(e,a)^c; o+=String.fromCharCode(l) } return t(o) } var o="/cdn-cgi/l/email-protection#", c=".__cf_email__", a="data-cfemail", l=document.createElement("div"); !function(){ for(var t=document.getElementsByTagName("a"),r=0;r<t.length;r++) try{ var c=t[r],a=c.href.indexOf(o); a>-1&&(c.href="mailto:"+n(c.href,a+o.length)) }catch(t){ e(t) } }(), function(){ for(var t=document.querySelectorAll(c),r=0;r<t.length;r++) try{ var o=t[r],l=n(o.getAttribute(a),0),i=document.createTextNode(l); o.parentNode.replaceChild(i,o) }catch(t){ e(t) } }() }); </script><div hidden><h2 class="sr-only">Templates (for web app):</h2><template id="_animation-template"><div class="animation-main fixed-top"><nav id="breadcrumbs" class="screen-only"><ul></ul></nav><div class="content"><div class="page"></div></div></div></template> <template id="_loading-template"><div class="loading nav-btn fr"> <span class="sr-only">Loading…</span> <span class="icon-cog"></span></div></template> <template id="_error-template"><div class="page"><h1 class="page-title">Error</h1><p class="lead"> Sorry, an error occurred while loading <a class="this-link" href=""></a>.</div></template> <template id="_permalink-template"> <a href="#" class="permalink"> <span class="sr-only">Permalink</span> <span class="content-hash"></span> </a> </template></div></html>
